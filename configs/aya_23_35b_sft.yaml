name: "aya-23-35b-custom-sft"
description: "Fine-tuning Aya-23-35B on a custom SFT dataset (Corrected Model Size: 35B as 32B does not exist)"
seed: 42

model:
  name: "aya-23-35b"  # Registered model name for Aya-23-35B
  model_name_or_path: "CohereForAI/aya-23-35B"
  model_type: "causal_lm"
  quantization: "4bit"  # Using 4-bit quantization for memory efficiency
  device_map: "auto"
  torch_dtype: "bfloat16"
  trust_remote_code: true
  use_flash_attention: true
  max_length: 4096
  use_unsloth: true  # Unsloth supports Aya models

lora:
  template: "default"
  r: 32  # Higher rank for larger model
  lora_alpha: 64
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  modules_to_save: null

training:
  output_dir: "./models/checkpoints/${name}"
  num_train_epochs: 3
  per_device_train_batch_size: 1  # Smaller batch size for 35B model
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16 # Increase accumulation to compensate for small batch size
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.05
  lr_scheduler_type: "cosine"
  logging_steps: 5
  eval_strategy: "steps"
  eval_steps: 50
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 2
  load_best_model_at_end: true
  gradient_checkpointing: true
  optim: "paged_adamw_32bit"
  max_grad_norm: 1.0
  use_unsloth: true

dataset:
  name: "custom_sft_dataset"
  source: "local"
  path: "./data/my_sft_dataset.json" # Place your dataset here
  split: "train"
  text_column: "text" # Ensure your JSON has this column
  # For instruction tuning (alpaca format):
  # prompt_column: "instruction"
  # response_column: "output"
  validation_split: 0.1
  test_split: 0.0
  max_samples: null
  seed: 42

tracker:
  type: "wandb"
  project: "arbox-aya-experiments"
  entity: null
  tags: ["aya-23", "35b", "lora", "custom-data"]
  notes: "Fine-tuning Aya 23 35B on custom data"
